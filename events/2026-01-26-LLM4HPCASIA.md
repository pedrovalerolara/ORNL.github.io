---
layout: default
title: LLM4HPCAsia 2026 
description: The 1st International Workshop on Foundational Large Language Models Advances for HPC in Asia<br />
             to be held in conjunction with SCA/HPC Asia 2026<br/>
             January 26th-29th, 2026<br />
             Osaka, Japan<br />
permalink: /events/llm4hpcasia2026/
tags: events
header-includes:
    - \usepackage{wallpaper}
---


<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<title>LLM4HPCAsia 2026</title>

<link rel="stylesheet" href="./2026-01-26-LLM4HPCASIA-files/llm4hpc2026asia.css" type="text/css">
</head>
<body>



<div class="header">
      <h1>LLM4HPCAsia 2026 </h1>
      <h2><font size="6">
The 1st International Workshop on Foundational large Language Models Advances for HPC in Asia</font></h2>
      <p>to be held in conjunction with<br>
	      <a href="https://www.sca-hpcasia2026.jp">SCA/HPC Asia 2026</a>
      </p><p>

      </p><p>
       26-29 January, 2026<br> Osaka, Japan</p>
</div>

<div class="body">


<h2>Introduction</h2>
<p>
Since their development and release, modern Large Language Models (LLMs), such as the Generative Pre-trained Transformer (GPT) model and the Large Language Model Meta AI (LLaMA), have come to signify a revolution in human-computer interaction spurred on by their high-quality results. LLMs have repaved this landscape thanks to unprecedented investments and enormous training models (hundreds of billions of parameters). The availability of LLMs has led to increasing interest in how they could be applied to a large variety of applications. The HPC community made recent research efforts to evaluate current LLM capabilities for some HPC tasks, including code generation, auto parallelization, performance portability, correctness, among others. All these studies concluded that state-of-the-art LLM capabilities have proven so far insufficient for these targets. Hence, it is necessary to explore novel techniques to further empower LLMs to enrich the HPC mission and its impact.
</p>

<h2>Call For Papers</h2>
<h2>Objectives, scope and topics of the workshop</h2>
<p>
This workshop objectives are focused on LLMs advances for any HPC major priority and challenge with the aims to define and discuss the fundamentals of LLMs for HPC-specific tasks, including but not limited to hardware design, compilation, parallel programming models and runtimes, application development, enabling LLM technologies to have more autonomous decision-making about the efficient use of HPC.

This workshop aims to provide a forum to discuss new and emerging solutions to address
these important challenges towards an AI-assisted HPC era. Papers are being sought on many
aspects of LLM for HPC targets including (but not limited to):
</p>
            <ul>
		<li>LLMs for Programming Environments and Runtime Systems</li>
		<li>LLMs for HPC and Scientific Applications</li>
	        <li>LLMs for Hardware design (including non-von Neumann Architectures)</li>
                <li>Reliability/Benchmarking/Measurements for LLMs</li>
            </ul>


<h2>Program</h2>
<p>
<strong>TBD</strong><br>
<!--
<strong>9,00AM-9,15AM</strong>: Opening Pedro Valero-Lara<br>
<strong>9,15AM-10,00AM</strong>: Keynote: LLM-enabled swarm intelligent agents for resilient HPC infrastructures, Prasanna Balaprakash<br>
<strong>10,00AM-10,30AM</strong>: First talk: Analysis of MPI Parallel Code Generated by GPT-4o, Rin Tanaka<br>
<strong>10,30AM-11,00AM</strong>: Second talk: LLM & HPC:Benchmarking DeepSeek's Performance in High-Performance Computing Tasks, Patrick Diehl<br>
<strong>11,00AM-11,30AM</strong>: Break<br>
<strong>11,30AM-12,00PM</strong>: Third talk: Leveraging AI for productive and trustworthy HPC software: challenges and research directions, Pedro Valero-Lara<br>
<strong>12,00PM- 1,00PM</strong>: Panel: LLM4HPC --Challenges and Opportunities<br>
<strong>Moderator</strong>: Daniel Lee Nichols<br>
<strong>Panelists</strong>: Jeffrey S. Vetter, Prasanna Balaprakash, Rin Tanaka, Patrick Diehl, Pedro Valero-Lara<br>	
-->
</p>


<h2>Important Dates</h2>
<p>
<strong>Paper submission deadline :</strong> Oct 20, 2025<br>
<strong>Notification of acceptance :</strong> Nov 26, 2025<br>
<!--
<strong>Camera-ready papers due :</strong> May 16, 2025<br>
<strong>Workshop day:</strong> June 13, 2025<br>
-->
</p>


<h2>Steering Committee</h2>
<p>
<strong>TBD</strong><br>
<!--
<strong>Jeffrey S. Vetter</strong>, Oak Ridge National Laboratory, USA<br>
</p>
<p>
<strong>Rosa M. Badia</strong>, Barcelona Supercomputing Center, Spain<br>
</p>
<p>
<strong>Franz Franchetti</strong>, Carnegie Mellon University, USA<br>
</p>
<p>
<strong>Enrique Quintana Orti</strong>, Universitat Politecnica de Valencia, Spain<br>
</p>
<p>
<strong>Abhinav Bhatele</strong>, University of Maryland, USA<br>
-->
</p>

<h2>Organizers (Contact us)</h2>

<p>
<strong>Pedro Valero-Lara (chair)</strong><br>
Oak Ridge National Laboratory, USA<br>
<i>valerolarap@ornl.gov</i>
</p>
<p>
<strong>William F. Godoy</strong><br>
Oak Ridge National Laboratory, USA<br>
<i>godoywf@ornl.gov</i>
</p>
<p>
<strong>Dhabaleswar K. Panda (co-chair)</strong><br>
The Ohio State University, USA<br>
<i>panda@cse.ohio-state.edu</i>
</p>

<h2>Programme Committee</h2>
<strong>TBD</strong><br>
<!--
<ul>
<li><strong>Samuel Williams</strong>, Lawrence Berkeley National Laboratory, USA</li>
<li><strong>Hiroyuki Takizawa</strong>, Tohoku University, Japan</li>
<li><strong>Gokcen Kestor</strong>, Barcelona Supercomputing Center, Spain</li>
<li><strong>Prasanna Balaprakash</strong>, Oak Ridge National Laboratory, USA</li>
<li><strong>Rabab Alomairy</strong>, Massachusetts Institute of Technology, USA</li>
<li><strong>Johannes Blaschke</strong>, Lawrence Berkeley National Laboratory, USA</li>
<li><strong>Ramakrishnan (Ramki) Kannan</strong>, Oak Ridge National Laboratory, USA</li>
<li><strong>Olivier Aumage</strong>, INRIA, France</li>
<li><strong>Ignacio Laguna</strong>, Lawrence Livermore National Laboratory, USA</li>
<li><strong>Johannes Doerfert</strong>, Lawrence Livermore National Laboratory, USA</li>
<li><strong>Monil Mohammad Alaul Haque</strong>, Oak Ridge National Laboratory, USA</li>
<li><strong>Simon Garcia De Gonzalo</strong>, Sandia National Laboratory, USA</li>
<li><strong>Diego Andrade Canosa</strong>, University of A Coruna, Spain</li>
<li><strong>Tze-Meng Low</strong>, Carnegie Mellon University, USA</li>
<li><strong>Dario Garcia Casulla</strong>, Barcelona Supercomputing Center, Spain</li>
<li><strong>Michel Schanen</strong>, Argonne National Laboratory, USA</li>
<li><strong>Keita Teranishi</strong>, Oak Ridge National Laboratory, USA</li>
<li><strong>William F. Godoy</strong>, Oak Ridge National Laboratory, USA</li>
<li><strong>Damian Rouson</strong>, Lawrence Berkeley National Laboratory, USA</li>
<li><strong>Jens Domke</strong>, RIKEN Center for Computational Science(R-CSS), Japan</li>
<li><strong>Narasinga Rao Minskar</strong>, Oak Ridge National Laboratory, USA</li>
<li><strong>Sunita Chandrasekaran</strong>, University of Delaware, USA</li>
<li><strong>Arjun Guha</strong>, Northeastern University, USA</li>
</ul>
-->

<h2>Manuscript submission</h2>
<strong>TBD</strong><br>
<!--
<p>
We invite submissions of original, unpublished research and experiential papers. 
Papers should be between <strong>6 to 12</strong> pages in length (including a bibliography and appendices, with two possible extra pages after the review to address the reviewer’s comments), formatted according to <a href="https://www.springer.com/de/it-informatik/lncs"> Springer’s Lecture Notes in Computer Science (LNCS)</a>. All paper submissions will be managed electronically via <a href="https://easychair.org/my/conference?conf=llm4hpc">EasyChair</a>.
</p>
-->

<h2>Proceedings</h2>
<strong>TBD</strong><br>
<!--
<p>
All accepted papers will be published in the ISC-HPC Workshops 2025 proceedings by SpringerLink. 
</p> 
-->

<h2>Best Paper Award</h2>
<p>
The Best Paper Award will be selected on the basis of explicit recommendations of the reviewers and their scoring towards the paper’s originality and quality. 
</p> 


<h2><strong>Keynote (Rio Yokota, Institute of Science Tokio):</strong></h2> <strong>Updates on the Development of Japanese LLMs</strong>
<p>
Large language models (LLM) are mainly pre-trained on internet data, which is predominantly English. Such models have suboptimal performance when used in non-English languages. Also, LLMs are not mechanical tools that benefit everyone equally. They are rather intellectual tools that disproportionately benefit certain groups of people, depending on what data they are trained on. Furthermore, the interaction with LLMs will influence our local culture in the long term. Sovereign LLMs are crucial for customizing the models to meet the needs of each local culture. In this talk I will give an update of the efforts in Japan to train LLMs. I will cover both the data and training aspects.
</p>
<p>
Rio Yokota is a Professor at the Supercomputing Research Center, Institute of Integrated Research, Institute of Science Tokyo. He also leads the AI for Science Foundation Model Research Team at RIKEN Center for Computational Science. His research interests lie at the intersection of high performance computing, machine learning, and linear algebra. He has been optimizing algorithms on GPUs since 2007, and was part of a team that received the Gordon Bell prize in 2009 using the first GPU supercomputer. More recently, he has been leading distributed training efforts on Japanese supercomputers such as ABCI, TSUBAME, and Fugaku. He is the co-developer of the Japanese LLM Swallow, and LLM-jp. He is also involved in the organization of multinational collaborations such as ADAC and TPC.
</p>


<h2><strong>Invited Talk (Min Si, Facebook AI):</strong></h2> <strong>High performance communication library and transport for LLM training at 100K+ Scale</strong>
<p>
Each successive generation of the LLaMA model has demonstrated substantial growth in both model size and complexity. The largest multimodal mixture-of-experts model within our LLaMA4 series possesses nearly two trillion total parameters, with 288 billion active parameters and 16 experts. To accommodate the computational demands associated with training such a colossal model, we expanded our AI clusters, deploying approximately 100,000 GPUs. GPU-to-GPU communication latency is a critical factor when coordinating such a vast number of GPUs. Even microsecond delays accumulate across thousands of nodes, consequently impacting the time required for training. We engineered the underlying network infrastructure to provide the necessary backbone for high-speed GPU-to-GPU communication, concurrently innovating our communication library stack to enhance overall communication efficiency. In this presentation, we will provide an overview of the network topology deployed within Meta datacenters and introduce a range of communication optimizations and custom features that facilitated LLaMA4 training through cross-layer codesign, encompassing model algorithms, collectives, and extending to the network transport layer.
</p>
<p>
Min Si is a Research Scientist at Facebook AI System SW/HW Co-design group. Her role is to investigate and resolve interesting scale-out challenges for Facebook AI workloads. Previously, she was an Assistant Computer Scientist at Argonne National Laboratory and working with the Programming Models and Runtime Systems group. Her research interests include communication runtime in high-performance computing, parallel programming models and runtime systems.
</p>

<p>
</p><h2>Registration</h2>
<p> 
Information about registration at <a href="https://www.sca-hpcasia2026.jp">SCA/HPC Asia 2026 website</a>.
</p>
